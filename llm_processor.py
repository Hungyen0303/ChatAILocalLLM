import os
import re
from llama_cpp import Llama
import logging
from typing import Tuple, Optional

# Import cáº¥u hÃ¬nh Ä‘Æ¡n giáº£n
from config import MODEL_DIR, MODEL_FILENAME, get_model_path

# Import MCP filesystem client
try:
    from mcp_client import process_filesystem_query, initialize_filesystem
    MCP_AVAILABLE = True
    logging.info("MCP Filesystem client loaded")
except ImportError as e:
    logging.warning(f"MCP Filesystem not available: {e}")
    MCP_AVAILABLE = False

logger = logging.getLogger(__name__)

# ÄÆ°á»ng dáº«n model tá»« config
MODEL_PATH = get_model_path()

# Kiá»ƒm tra file mÃ´ hÃ¬nh
if not os.path.exists(MODEL_PATH):
    raise FileNotFoundError(f"Model file not found at: {MODEL_PATH}")

# Khá»Ÿi táº¡o mÃ´ hÃ¬nh vá»›i cÃ¡c tham sá»‘ tá»‘i Æ°u
try:
    llm = Llama(
        model_path=MODEL_PATH,
        n_ctx=2048,         # Tá»‘i Æ°u context window
        n_threads=8,        # TÄƒng threads cho performance
        n_batch=256,        # TÄƒng batch size
        use_mlock=True,     # Lock memory Ä‘á»ƒ trÃ¡nh swap
        verbose=False,
        chat_format="llama-3"
    )
    logger.info(f"Model loaded successfully: {MODEL_FILENAME}")
except Exception as e:
    logger.error(f"Error loading model: {e}")
    raise

# Khá»Ÿi táº¡o MCP Filesystem
if MCP_AVAILABLE:
    try:
        initialize_filesystem()
        logger.info("MCP Filesystem initialized successfully")
    except Exception as e:
        logger.warning(f"MCP Filesystem initialization failed: {e}")
        MCP_AVAILABLE = False

# =============================================================================
# PATTERN DETECTION & INTENT RECOGNITION
# =============================================================================

def detect_intent_and_extract(prompt: str) -> Tuple[str, str, str]:
    """
    PhÃ¡t hiá»‡n intent vÃ  trÃ­ch xuáº¥t dá»¯ liá»‡u tá»« prompt sá»­ dá»¥ng patterns
    
    Args:
        prompt (str): CÃ¢u lá»‡nh tá»« ngÆ°á»i dÃ¹ng
        
    Returns:
        Tuple[str, str, str]: (intent, query, original_prompt)
    """
    prompt_lower = prompt.lower().strip()
    
    # Enhanced search patterns vá»›i nhiá»u biáº¿n thá»ƒ
    search_patterns = [
        # Quoted search terms
        (r'tÃ¬m file.*?["\']([^"\']+)["\']', 'search'),
        (r'tÃ¬m.*?file.*?["\']([^"\']+)["\']', 'search'),
        (r'file.*?cÃ³.*?["\']([^"\']+)["\']', 'search'),
        (r'search.*?["\']([^"\']+)["\']', 'search'),
        
        # Natural language patterns
        (r'(?:tÃ´i )?(?:cáº§n )?(?:muá»‘n )?tÃ¬m file\s+(.+?)(?:\s+trong|\s*$)', 'search'),
        (r'(?:tÃ´i )?(?:cáº§n )?(?:muá»‘n )?tÃ¬m.*?file.*?(\w.+?)(?:\s+trong|\s*$)', 'search'),
        (r'(?:tÃ´i )?(?:cáº§n )?(?:muá»‘n )?search.*?file.*?(\w.+?)(?:\s+trong|\s*$)', 'search'),
        (r'file.*?cÃ³.*?(\w.+?)(?:\s+khÃ´ng|\s*$)', 'search'),
        (r'cÃ³ file.*?vá»\s+(.+?)(?:\s*$)', 'search'),
        (r'(?:tÃ´i )?(?:cáº§n )?tÃ¬m\s+(.+?)(?:\s+file|\s*$)', 'search'),
        
        # More flexible patterns
        (r'(?:tÃ´i )?(?:cáº§n )?(?:muá»‘n )?(?:tÃ¬m|search|tim)\s+(?:file\s+)?(.+?)(?:\s+(?:trong|á»Ÿ|táº¡i)|\s*$)', 'search'),
    ]
    
    # Check search patterns first
    for pattern, intent in search_patterns:
        match = re.search(pattern, prompt_lower)
        if match:
            query = match.group(1).strip()
            # Clean up extracted query
            query = clean_extracted_query(query)
            if query and len(query) > 1:
                return intent, query, prompt
    
    # Simple intent patterns
    if any(word in prompt_lower for word in ['quÃ©t', 'scan', 'liá»‡t kÃª', 'hiá»ƒn thá»‹ file', 'danh sÃ¡ch']):
        return 'scan', '', prompt
    
    if any(word in prompt_lower for word in ['phÃ¢n loáº¡i', 'classify', 'nhÃ³m', 'categorize', 'sáº¯p xáº¿p']):
        return 'classify', '', prompt
    
    if any(word in prompt_lower for word in ['xuáº¥t', 'export', 'metadata', 'gá»­i cloud']):
        return 'export', '', prompt
    
    # Fallback for search without clear patterns
    search_keywords = ['tÃ¬m', 'search', 'file', 'tÃ i liá»‡u', 'tim']
    if any(keyword in prompt_lower for keyword in search_keywords):
        # Extract search terms by removing common words
        stop_words = {'tÃ´i', 'cáº§n', 'muá»‘n', 'tÃ¬m', 'tim', 'file', 'cÃ³', 'nÃ o', 'kiáº¿m', 'tÃ i', 'liá»‡u', 'vá»', 'trong', 'search', 'á»Ÿ', 'táº¡i'}
        words = [w for w in prompt.split() if w.lower() not in stop_words and len(w) > 1]
        if words:
            query = ' '.join(words[:5])  # Take first 5 meaningful words
            return 'search', query, prompt
    
    return 'general', '', prompt

def clean_extracted_query(query: str) -> str:
    """
    LÃ m sáº¡ch query Ä‘Æ°á»£c trÃ­ch xuáº¥t tá»« prompt
    
    Args:
        query (str): Query thÃ´ cáº§n lÃ m sáº¡ch
        
    Returns:
        str: Query Ä‘Ã£ Ä‘Æ°á»£c lÃ m sáº¡ch
    """
    if not query:
        return ""
    
    # Remove trailing words that are not part of search terms
    cleanup_suffixes = ['trong', 'á»Ÿ', 'táº¡i', 'khÃ´ng', 'nÃ o', 'gÃ¬', 'Ä‘Ã¢u']
    words = query.split()
    
    # Remove suffix words
    while words and words[-1].lower() in cleanup_suffixes:
        words.pop()
    
    # Remove prefix words if they exist
    cleanup_prefixes = ['file', 'tÃ i', 'liá»‡u']
    while words and words[0].lower() in cleanup_prefixes:
        words.pop(0)
    
    result = ' '.join(words).strip()
    
    # Validate result has meaningful content
    if len(result) > 1 and any(c.isalnum() for c in result):
        return result
    
    return query.strip()  # Return original if cleaning failed

# =============================================================================
# RESULT FORMATTING
# =============================================================================

def format_mcp_result(result: str, intent: str, query: str = '', original_prompt: str = '') -> str:
    """
    Format káº¿t quáº£ MCP vá»›i cáº£i tiáº¿n hiá»ƒn thá»‹
    
    Args:
        result (str): Káº¿t quáº£ thÃ´ tá»« MCP
        intent (str): Intent Ä‘Æ°á»£c phÃ¡t hiá»‡n
        query (str): Query tÃ¬m kiáº¿m (náº¿u cÃ³)
        original_prompt (str): Prompt gá»‘c
        
    Returns:
        str: Káº¿t quáº£ Ä‘Ã£ Ä‘Æ°á»£c format Ä‘áº¹p
    """
    
    if intent == 'search':
        if 'KhÃ´ng tÃ¬m tháº¥y' in result or 'No files found' in result or not result.strip():
            return f"ğŸ” KhÃ´ng tÃ¬m tháº¥y file nÃ o vá»›i tá»« khÃ³a '{query}'"
        
        # Clean and process the result for better presentation
        cleaned_result = clean_search_result(result, query)
        
        # Count files from cleaned result to ensure consistency
        file_count = count_files_in_text(cleaned_result)
        
        if file_count == 0:
            return f"ğŸ” KhÃ´ng cÃ³ káº¿t quáº£ tÃ¬m kiáº¿m cho '{query}'\n\nğŸ“„ Dá»¯ liá»‡u tráº£ vá»:\n{result}"
        elif file_count == 1:
            return f"âœ… TÃ¬m tháº¥y 1 file vá»›i tá»« khÃ³a '{query}':\n\n{cleaned_result}"
        else:
            return f"âœ… TÃ¬m tháº¥y {file_count} file vá»›i tá»« khÃ³a '{query}':\n\n{cleaned_result}"
    
    elif intent == 'scan':
        # Check for successful scan - look for actual data instead of "success" word
        has_data = any(indicator in result for indicator in [
            'NhÃ³m', 'file', 'Filename:', 'Categories', 'â€¢', '-', 'Ä‘Ã£ Ä‘Æ°á»£c', 'Total'
        ])
        
        if has_data and len(result.strip()) > 10:
            # Count files by multiple methods
            file_count = 0
            
            # Method 1: Count by patterns
            file_patterns = [r'Filename:', r'â€¢ NhÃ³m.*?(\d+)\s+file', r'(\d+)\s+file']
            for pattern in file_patterns:
                matches = re.findall(pattern, result)
                if matches:
                    if pattern == r'Filename:':
                        file_count = len(matches)
                    else:
                        # Sum up numbers found
                        file_count = sum(int(match) if isinstance(match, str) and match.isdigit() 
                                       else int(match[0]) if isinstance(match, tuple) and match[0].isdigit() 
                                       else 0 for match in matches)
                    break
            
            return f"ğŸ“‚ QuÃ©t thÆ° má»¥c hoÃ n thÃ nh\nğŸ“Š Tá»•ng sá»‘ file: {file_count}\n\n{result}"
        else:
            return f"âŒ Lá»—i quÃ©t thÆ° má»¥c: {result}"
    
    elif intent == 'classify':
        # Check for classification data
        has_classification = any(indicator in result for indicator in [
            'NhÃ³m', 'Categories', 'phÃ¢n loáº¡i', 'file', 'â€¢'
        ])
        
        if has_classification:
            return f"ğŸ“‹ PhÃ¢n loáº¡i file thÃ nh cÃ´ng:\n\n{result}"
        else:
            return f"âŒ Lá»—i phÃ¢n loáº¡i: {result}"
    
    elif intent == 'export':
        # Check if export was successful and has detailed data
        has_export_data = any(indicator in result for indicator in [
            'Filename:', 'File:', 'size:', 'bytes', 'NhÃ³m', 'metadata', 'â€¢', '-'
        ])
        
        if has_export_data and len(result.strip()) > 50:
            # Format detailed metadata export
            formatted_export = format_export_metadata(result)
            return f"ğŸ“¤ Xuáº¥t metadata thÃ nh cÃ´ng\n\n{formatted_export}"
        elif 'success' in result.lower() or 'exported' in result.lower() or 'xuáº¥t' in result.lower():
            return f"âœ… Xuáº¥t metadata thÃ nh cÃ´ng\nğŸ“¦ Dá»¯ liá»‡u Ä‘Ã£ sáºµn sÃ ng\n\n{result}"
        else:
            return f"âŒ Lá»—i xuáº¥t metadata: {result}"
    
    return result

def format_export_metadata(result: str) -> str:
    """
    Format metadata export Ä‘á»ƒ hiá»ƒn thá»‹ thÃ´ng tin chi tiáº¿t cho tá»«ng file
    
    Args:
        result (str): Káº¿t quáº£ export thÃ´
        
    Returns:
        str: Metadata Ä‘Ã£ Ä‘Æ°á»£c format Ä‘áº¹p
    """
    
    lines = result.strip().split('\n')
    formatted_lines = []
    
    # Add header
    formatted_lines.append("ğŸ“‹ CHI TIáº¾T METADATA CÃC FILE:")
    formatted_lines.append("=" * 60)
    
    file_count = 0
    current_file = {}
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        # Skip summary lines
        if any(skip_phrase in line.lower() for skip_phrase in [
            'Ä‘Ã£ xuáº¥t metadata', 'sáºµn sÃ ng gá»­i', 'Ä‘Ã£ Ä‘Æ°á»£c phÃ¢n loáº¡i', 'chuáº©n bá»‹ metadata'
        ]):
            continue
            
        # Detect file entries
        if line.startswith('â€¢') or line.startswith('-') or 'Filename:' in line:
            # Save previous file if exists
            if current_file:
                formatted_lines.append(format_single_file_metadata(current_file, file_count))
                current_file = {}
            
            file_count += 1
            
            # Extract filename and basic info
            if line.startswith('â€¢') or line.startswith('-'):
                # Format: â€¢ filename.ext (Category) - size
                match = re.search(r'[â€¢-]\s*(.+?)\s*(?:\(([^)]+)\))?\s*(?:-\s*(\d+\s*bytes?))?', line)
                if match:
                    filename = match.group(1).strip()
                    category = match.group(2).strip() if match.group(2) else "ChÆ°a phÃ¢n loáº¡i"
                    size = match.group(3).strip() if match.group(3) else "Unknown"
                    
                    current_file = {
                        'filename': filename,
                        'category': category,
                        'size': size,
                        'raw_line': line
                    }
            elif 'Filename:' in line:
                filename = line.replace('Filename:', '').strip()
                current_file = {'filename': filename}
                
        # Extract additional metadata fields
        elif ':' in line and current_file:
            key, value = line.split(':', 1)
            key = key.strip().lower()
            value = value.strip()
            
            if key in ['size', 'kÃ­ch thÆ°á»›c']:
                current_file['size'] = value
            elif key in ['category', 'nhÃ³m', 'phÃ¢n loáº¡i']:
                current_file['category'] = value
            elif key in ['path', 'Ä‘Æ°á»ng dáº«n']:
                current_file['path'] = value
            elif key in ['type', 'loáº¡i']:
                current_file['type'] = value
            elif key in ['modified', 'sá»­a Ä‘á»•i', 'last modified']:
                current_file['modified'] = value
            elif key in ['created', 'táº¡o', 'created date']:
                current_file['created'] = value
    
    # Add the last file
    if current_file:
        formatted_lines.append(format_single_file_metadata(current_file, file_count))
    
    # Add summary
    formatted_lines.append("=" * 60)
    formatted_lines.append(f"ğŸ“Š Tá»”NG Káº¾T: {file_count} file Ä‘Ã£ Ä‘Æ°á»£c xuáº¥t metadata")
    formatted_lines.append("âœ… Sáºµn sÃ ng gá»­i Ä‘áº¿n MCP Cloud Storage")
    
    return '\n'.join(formatted_lines)

def format_single_file_metadata(file_data: dict, file_num: int) -> str:
    """
    Format metadata cho má»™t file riÃªng láº»
    
    Args:
        file_data (dict): Dá»¯ liá»‡u metadata cá»§a file
        file_num (int): Sá»‘ thá»© tá»± file
        
    Returns:
        str: Metadata file Ä‘Ã£ Ä‘Æ°á»£c format
    """
    
    lines = []
    lines.append(f"\nğŸ“„ FILE #{file_num}:")
    lines.append("-" * 40)
    
    # Essential fields
    if 'filename' in file_data:
        lines.append(f"ğŸ“ TÃªn file: {file_data['filename']}")
    
    if 'category' in file_data:
        lines.append(f"ğŸ“‚ PhÃ¢n loáº¡i: {file_data['category']}")
    
    if 'size' in file_data:
        lines.append(f"ğŸ’¾ KÃ­ch thÆ°á»›c: {file_data['size']}")
    
    # Optional fields
    if 'type' in file_data:
        lines.append(f"ğŸ”§ Loáº¡i file: {file_data['type']}")
    
    if 'path' in file_data:
        lines.append(f"ğŸ“ ÄÆ°á»ng dáº«n: {file_data['path']}")
    
    if 'modified' in file_data:
        lines.append(f"â° Sá»­a Ä‘á»•i: {file_data['modified']}")
    
    if 'created' in file_data:
        lines.append(f"ğŸ†• Táº¡o lÃºc: {file_data['created']}")
    
    # Add raw line if no structured data
    if len(file_data) <= 2 and 'raw_line' in file_data:
        lines.append(f"ğŸ“‹ ThÃ´ng tin: {file_data['raw_line']}")
    
    return '\n'.join(lines)

# =============================================================================
# UTILITY FUNCTIONS
# =============================================================================

def count_files_in_text(text: str) -> int:
    """Count unique files in text - consistent with cleaning logic"""
    
    lines = text.strip().split('\n')
    unique_files = set()
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        # Look for file entries (lines starting with â€¢ or -)
        if line.startswith('â€¢') or line.startswith('-'):
            # Extract filename for counting
            filename_match = re.search(r'([^/\\]+\.(txt|pdf|docx|pptx|doc|ppt|xlsx|csv))', line, re.IGNORECASE)
            if filename_match:
                filename = filename_match.group(1).strip()
                unique_files.add(filename)
        # Also check lines that contain file extensions but don't start with â€¢ or -
        elif any(ext in line.lower() for ext in ['.txt', '.pdf', '.docx', '.pptx', '.doc', '.ppt', '.xlsx', '.csv']):
            filename_match = re.search(r'([^/\\]+\.(txt|pdf|docx|pptx|doc|ppt|xlsx|csv))', line, re.IGNORECASE)
            if filename_match:
                filename = filename_match.group(1).strip()
                unique_files.add(filename)
    
    return len(unique_files)

def clean_search_result(result: str, query: str) -> str:
    """Clean and improve search result presentation"""
    
    # Remove redundant title if exists
    lines = result.strip().split('\n')
    cleaned_lines = []
    
    seen_files = set()
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        # Skip redundant title lines that contain the query and end with ':'
        if ("TÃ¬m tháº¥y" in line and query in line and line.endswith(':')) or \
           ("file cÃ³" in line and query in line and line.endswith(':')):
            continue
            
        # Skip lines that just repeat the search
        if line == f"TÃ¬m tháº¥y file cÃ³ '{query}':":
            continue
        
        # Handle file entries (lines starting with â€¢ or -)
        if line.startswith('â€¢') or line.startswith('-'):
            # Extract filename for duplicate detection
            filename_match = re.search(r'([^/\\]+\.(txt|pdf|docx|pptx|doc|ppt|xlsx|csv))', line, re.IGNORECASE)
            if filename_match:
                filename = filename_match.group(1).strip()
                if filename in seen_files:
                    continue  # Skip duplicate
                seen_files.add(filename)
        
        # Handle other lines that might contain file info
        elif any(ext in line.lower() for ext in ['.txt', '.pdf', '.docx', '.pptx', '.doc', '.ppt', '.xlsx', '.csv']):
            filename_match = re.search(r'([^/\\]+\.(txt|pdf|docx|pptx|doc|ppt|xlsx|csv))', line, re.IGNORECASE)
            if filename_match:
                filename = filename_match.group(1).strip()
                if filename in seen_files:
                    continue  # Skip duplicate
                seen_files.add(filename)
        
        cleaned_lines.append(line)
    
    return '\n'.join(cleaned_lines)

def generate_simple_response(prompt: str) -> str:
    """Generate simple LLM response - only for general chat"""
    try:
        response = llm.create_chat_completion(
            messages=[
                {"role": "system", "content": "Báº¡n lÃ  trá»£ lÃ½ AI. Tráº£ lá»i ngáº¯n gá»n."},
                {"role": "user", "content": prompt}
            ],
            max_tokens=300,
            temperature=0.7,
            stop=["\n\n"]
        )
        
        content = response["choices"][0]["message"]["content"].strip()
        return content if content else "Xin lá»—i, tÃ´i khÃ´ng hiá»ƒu."
        
    except Exception as e:
        logger.error(f"LLM error: {e}")
        return f"Lá»—i LLM: {str(e)}"

def handle_error_with_pattern(error: str, intent: str, original_prompt: str = '') -> str:
    """Handle errors using patterns - preserve context"""
    
    error_patterns = {
        'file not found': 'KhÃ´ng tÃ¬m tháº¥y file hoáº·c thÆ° má»¥c',
        'permission denied': 'KhÃ´ng cÃ³ quyá»n truy cáº­p file',
        'connection': 'Lá»—i káº¿t ná»‘i MCP',
        'timeout': 'Háº¿t thá»i gian chá»',
        'invalid': 'Dá»¯ liá»‡u khÃ´ng há»£p lá»‡'
    }
    
    error_lower = error.lower()
    for pattern, vietnamese_msg in error_patterns.items():
        if pattern in error_lower:
            if intent == 'search':
                return f"Lá»—i tÃ¬m kiáº¿m: {vietnamese_msg}"
            elif intent == 'scan':
                return f"Lá»—i quÃ©t thÆ° má»¥c: {vietnamese_msg}"
            elif intent == 'classify':
                return f"Lá»—i phÃ¢n loáº¡i: {vietnamese_msg}"
            elif intent == 'export':
                return f"Lá»—i xuáº¥t file: {vietnamese_msg}"
    
    return f"Lá»—i há»‡ thá»‘ng: {error}"

def process_prompt(prompt: str) -> str:
    """Main prompt processing - preserve original context"""
    
    # Detect intent and extract data using patterns - get original prompt back
    intent, query, original_prompt = detect_intent_and_extract(prompt)
    
    # Process with MCP if available and not general chat
    if MCP_AVAILABLE and intent != 'general':
        try:
            # Call appropriate MCP function
            if intent == 'search':
                mcp_result = process_filesystem_query(query, "search")
                logger.info(f"Search '{query}' returned: {mcp_result[:200]}...")
            elif intent == 'scan':
                mcp_result = process_filesystem_query("", "scan")
                logger.info(f"Scan returned: {mcp_result[:200]}...")
            elif intent == 'classify':
                mcp_result = process_filesystem_query("", "classify")
                logger.info(f"Classify returned: {mcp_result[:200]}...")
            elif intent == 'export':
                mcp_result = process_filesystem_query("", "export")
                logger.info(f"Export returned: {mcp_result[:200]}...")
            else:
                return generate_simple_response(original_prompt)
            
            # Format result using patterns (no LLM) - pass original prompt
            formatted_result = format_mcp_result(mcp_result, intent, query, original_prompt)
            logger.info(f"Formatted result: {formatted_result[:100]}...")
            return formatted_result
                
        except Exception as e:
            logger.error(f"MCP error: {e}")
            # Handle error with patterns (no LLM) - preserve context
            return handle_error_with_pattern(str(e), intent, original_prompt)
    
    # Only use LLM for general chat - use original prompt
    return generate_simple_response(original_prompt)